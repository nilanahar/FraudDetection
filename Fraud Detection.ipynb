{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A data science project of credit cards transactions dataset with insights, regression analysis and prediction models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is a simulated credit card transaction dataset containing legitimate and fraud transactions from the duration 1st Jan 2019 - 31st Dec 2020. It covers credit cards of 1000 customers doing transactions with a pool of 800 merchants. The dataset was generated using Sparkov Data Generation | Github tool created by Brandon Harris. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages \n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [12, 12]\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has already been separated and saved in two .csv files for train and test data. However, we will concat into one data set and build own our train and test sets using randomization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data sets:\n",
    "\n",
    "# building dataset\n",
    "temp_train = pd.read_csv('fraudTrain.csv')\n",
    "\n",
    "# validating dataset\n",
    "temp_test = pd.read_csv('fraudTest.csv')\n",
    "\n",
    "# concating dataset into one: \n",
    "df = pd.concat([temp_train, temp_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>trans_date_trans_time</th>\n",
       "      <th>cc_num</th>\n",
       "      <th>merchant</th>\n",
       "      <th>category</th>\n",
       "      <th>amt</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>gender</th>\n",
       "      <th>street</th>\n",
       "      <th>...</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>job</th>\n",
       "      <th>dob</th>\n",
       "      <th>trans_num</th>\n",
       "      <th>unix_time</th>\n",
       "      <th>merch_lat</th>\n",
       "      <th>merch_long</th>\n",
       "      <th>is_fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-01 00:00:18</td>\n",
       "      <td>2703186189652095</td>\n",
       "      <td>fraud_Rippin, Kub and Mann</td>\n",
       "      <td>misc_net</td>\n",
       "      <td>4.97</td>\n",
       "      <td>Jennifer</td>\n",
       "      <td>Banks</td>\n",
       "      <td>F</td>\n",
       "      <td>561 Perry Cove</td>\n",
       "      <td>...</td>\n",
       "      <td>36.0788</td>\n",
       "      <td>-81.1781</td>\n",
       "      <td>3495</td>\n",
       "      <td>Psychologist, counselling</td>\n",
       "      <td>1988-03-09</td>\n",
       "      <td>0b242abb623afc578575680df30655b9</td>\n",
       "      <td>1325376018</td>\n",
       "      <td>36.011293</td>\n",
       "      <td>-82.048315</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-01 00:00:44</td>\n",
       "      <td>630423337322</td>\n",
       "      <td>fraud_Heller, Gutmann and Zieme</td>\n",
       "      <td>grocery_pos</td>\n",
       "      <td>107.23</td>\n",
       "      <td>Stephanie</td>\n",
       "      <td>Gill</td>\n",
       "      <td>F</td>\n",
       "      <td>43039 Riley Greens Suite 393</td>\n",
       "      <td>...</td>\n",
       "      <td>48.8878</td>\n",
       "      <td>-118.2105</td>\n",
       "      <td>149</td>\n",
       "      <td>Special educational needs teacher</td>\n",
       "      <td>1978-06-21</td>\n",
       "      <td>1f76529f8574734946361c461b024d99</td>\n",
       "      <td>1325376044</td>\n",
       "      <td>49.159047</td>\n",
       "      <td>-118.186462</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-01-01 00:00:51</td>\n",
       "      <td>38859492057661</td>\n",
       "      <td>fraud_Lind-Buckridge</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>220.11</td>\n",
       "      <td>Edward</td>\n",
       "      <td>Sanchez</td>\n",
       "      <td>M</td>\n",
       "      <td>594 White Dale Suite 530</td>\n",
       "      <td>...</td>\n",
       "      <td>42.1808</td>\n",
       "      <td>-112.2620</td>\n",
       "      <td>4154</td>\n",
       "      <td>Nature conservation officer</td>\n",
       "      <td>1962-01-19</td>\n",
       "      <td>a1a22d70485983eac12b5b88dad1cf95</td>\n",
       "      <td>1325376051</td>\n",
       "      <td>43.150704</td>\n",
       "      <td>-112.154481</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2019-01-01 00:01:16</td>\n",
       "      <td>3534093764340240</td>\n",
       "      <td>fraud_Kutch, Hermiston and Farrell</td>\n",
       "      <td>gas_transport</td>\n",
       "      <td>45.00</td>\n",
       "      <td>Jeremy</td>\n",
       "      <td>White</td>\n",
       "      <td>M</td>\n",
       "      <td>9443 Cynthia Court Apt. 038</td>\n",
       "      <td>...</td>\n",
       "      <td>46.2306</td>\n",
       "      <td>-112.1138</td>\n",
       "      <td>1939</td>\n",
       "      <td>Patent attorney</td>\n",
       "      <td>1967-01-12</td>\n",
       "      <td>6b849c168bdad6f867558c3793159a81</td>\n",
       "      <td>1325376076</td>\n",
       "      <td>47.034331</td>\n",
       "      <td>-112.561071</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2019-01-01 00:03:06</td>\n",
       "      <td>375534208663984</td>\n",
       "      <td>fraud_Keeling-Crist</td>\n",
       "      <td>misc_pos</td>\n",
       "      <td>41.96</td>\n",
       "      <td>Tyler</td>\n",
       "      <td>Garcia</td>\n",
       "      <td>M</td>\n",
       "      <td>408 Bradley Rest</td>\n",
       "      <td>...</td>\n",
       "      <td>38.4207</td>\n",
       "      <td>-79.4629</td>\n",
       "      <td>99</td>\n",
       "      <td>Dance movement psychotherapist</td>\n",
       "      <td>1986-03-28</td>\n",
       "      <td>a41d7549acf90789359a9aa5346dcb46</td>\n",
       "      <td>1325376186</td>\n",
       "      <td>38.674999</td>\n",
       "      <td>-78.632459</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 trans_date_trans_time            cc_num  \\\n",
       "0           0   2019-01-01 00:00:18  2703186189652095   \n",
       "1           1   2019-01-01 00:00:44      630423337322   \n",
       "2           2   2019-01-01 00:00:51    38859492057661   \n",
       "3           3   2019-01-01 00:01:16  3534093764340240   \n",
       "4           4   2019-01-01 00:03:06   375534208663984   \n",
       "\n",
       "                             merchant       category     amt      first  \\\n",
       "0          fraud_Rippin, Kub and Mann       misc_net    4.97   Jennifer   \n",
       "1     fraud_Heller, Gutmann and Zieme    grocery_pos  107.23  Stephanie   \n",
       "2                fraud_Lind-Buckridge  entertainment  220.11     Edward   \n",
       "3  fraud_Kutch, Hermiston and Farrell  gas_transport   45.00     Jeremy   \n",
       "4                 fraud_Keeling-Crist       misc_pos   41.96      Tyler   \n",
       "\n",
       "      last gender                        street  ...      lat      long  \\\n",
       "0    Banks      F                561 Perry Cove  ...  36.0788  -81.1781   \n",
       "1     Gill      F  43039 Riley Greens Suite 393  ...  48.8878 -118.2105   \n",
       "2  Sanchez      M      594 White Dale Suite 530  ...  42.1808 -112.2620   \n",
       "3    White      M   9443 Cynthia Court Apt. 038  ...  46.2306 -112.1138   \n",
       "4   Garcia      M              408 Bradley Rest  ...  38.4207  -79.4629   \n",
       "\n",
       "   city_pop                                job         dob  \\\n",
       "0      3495          Psychologist, counselling  1988-03-09   \n",
       "1       149  Special educational needs teacher  1978-06-21   \n",
       "2      4154        Nature conservation officer  1962-01-19   \n",
       "3      1939                    Patent attorney  1967-01-12   \n",
       "4        99     Dance movement psychotherapist  1986-03-28   \n",
       "\n",
       "                          trans_num   unix_time  merch_lat  merch_long  \\\n",
       "0  0b242abb623afc578575680df30655b9  1325376018  36.011293  -82.048315   \n",
       "1  1f76529f8574734946361c461b024d99  1325376044  49.159047 -118.186462   \n",
       "2  a1a22d70485983eac12b5b88dad1cf95  1325376051  43.150704 -112.154481   \n",
       "3  6b849c168bdad6f867558c3793159a81  1325376076  47.034331 -112.561071   \n",
       "4  a41d7549acf90789359a9aa5346dcb46  1325376186  38.674999  -78.632459   \n",
       "\n",
       "   is_fraud  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a peak into data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1852394, 23)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of rows and columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cleaning dataset ##\n",
    "\n",
    "# rename columns\n",
    "df = df.rename(columns={'trans_date_trans_time': 'Date/Time', 'cc_num':'card_num','first':'first_name','last':'last_name','dob':'DOB','trans_num':'trans_ID','is_fraud':'fraud'})\n",
    "\n",
    "# cleaning the merchant column\n",
    "df['merchant']=df['merchant'].str.replace('fraud_','')\n",
    "\n",
    "# transforming date time\n",
    "df['Date/Time'] = pd.to_datetime(df['Date/Time'])\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating a rule for reporting all fraud trans: ##\n",
    "\n",
    "# creating a list\n",
    "bool1=[]\n",
    "# traversing through dataframe\n",
    "for trans in df.fraud:\n",
    "    if trans == 1:\n",
    "        bool1.append(True)\n",
    "    else:\n",
    "        bool1.append(False)\n",
    "\n",
    "# creating a panda series\n",
    "bool1=pd.Series(bool1)\n",
    "\n",
    "fraud = pd.DataFrame(df[bool1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some simple graphs to visualize data and pull some insights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transactions and months/season:\n",
    "\n",
    "# deriving month:\n",
    "df['month'] = df['Date/Time'].dt.month_name()\n",
    "\n",
    "# plot\n",
    "df['month'].value_counts().plot.bar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize fraud transactions vs valid transactions\n",
    "sns.countplot('fraud', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know from most transactional data analysis, the percentage of fraud transactions is usually low. The graph confirms this. However, a low percentage of fraud transactions makes it very difficult to classify data or build models. This is due to lack of fraud data points to achieve accuracy and validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fraud transactions happening per month: \n",
    "\n",
    "# derive month\n",
    "fraud['month'] = fraud['Date/Time'].dt.month_name()\n",
    "# plot\n",
    "fraud['month'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! Most fraud transactions occur in March. I thought it would be December due to winter holidays season. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean amount of fraud happening:\n",
    "fraud['amt'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max amount of fraud happening:\n",
    "fraud['amt'].max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is quite a low number for most banking transactions. The data advises that there are more small number fraud transactions than really large amounts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fraud and category\n",
    "sns.countplot(x='fraud',hue='category',data=fraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# highest fraud mean among categories:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that internet and grocery shopping has the most fraud transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation between variables \n",
    "corr = df.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm', axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better visualization if you are not a number's person: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n",
    "            square=True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph shows that most of the variables are not correlated. Some very common ones are like merchant's location with zip code and customer's location. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do some data analysis and build some prediction models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we will use selective variables which we are interested in. We want to see if the chosen variables can predic if a transaction is fraud or not. The chosen variables are category, amt, gender, lat, long, city_pop, merch_lat, merch_long. We need to change these variables into binary dummy variables for our analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dummy variables\n",
    "category_d = pd.get_dummies(df['category'],drop_first=True)\n",
    "gender_d = pd.get_dummies(df['gender'],drop_first=True)\n",
    "\n",
    "# adding the new dummy ones\n",
    "df = pd.concat([df,category_d,gender_d],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# creating a back up\n",
    "temp_df= df\n",
    "\n",
    "# dependent variable\n",
    "y = temp_df['fraud']\n",
    "\n",
    "# easier to drop columns than name the necessary ones, predictor variable\n",
    "X = temp_df.drop(['Date/Time', 'card_num', 'merchant', 'category',\n",
    "       'first_name', 'last_name', 'gender', 'street', 'city', 'state', 'zip',\n",
    "       'job', 'DOB', 'trans_ID', 'unix_time', 'fraud', 'month'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classifier using Logistic Regression ##\n",
    "\n",
    "# split data\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=0)\n",
    "\n",
    "# implementing the model\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# predictions\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# confusion matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have gotten our data to fit a model. Now, we will check for accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix tells us that 460447 are correct predictions, while 215 + 2437 are incorrecr predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at some accuracy graphs. ROC is a common tool in statistical analysis concerning binary classifiers. The line represents the ROC curve of a purely random classifier; we want our classifier to be as far away from the line as possible (toward the top-left corner) to show efficiency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "logit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC area is 0.50, it could be better. Some of the backdraws of this analysis is the data is highly imbalanced, since the difference between percentages of fraud transactions vs. valid transactions is relatively high. For next time, we would need more a stronger data set. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
